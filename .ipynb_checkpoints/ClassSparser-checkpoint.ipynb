{
 "metadata": {
  "name": "",
  "signature": "sha256:4d039243dd45f8d17f3f8eb525cf5c421248f1257b2b1620254c7a95e916e198"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Class Sparser Method\n",
      "This document describes a novel methodology for attribute space deformation to improve performance on classification tasks.\n",
      "There are some strategies for transforming input data space in order to make new attributes over which a given classifier could improve test performance.\n",
      "\n",
      "Actual strategies include semi supervized input space stransformation, as Restricted Boltzmann Machines (the main basic units of Deep Belief Networks), or totally unsupervized strategies as the use of kernel functions for nonlinear transformation of input space onto usually higher dimmensional spaces. This last idea behind kernel based methods of applying an arbitrary function to transform original space in order to make a space easier to learn by simpler models is taken to the extreme by Extreme Learning Machines. Another remarkable (unsupervized) space transformation technique is T-SNE, which in practice seems to create additional clusters.\n",
      "\n",
      "An open question remaining is which is the best strategy to transform input space that ensures a better fit of some (or any) classifier. On classification tasks, both, input space and class assignments are given, so, class information can always be included on the space transformation tuning step.\n",
      "\n",
      "## Outline of the method\n",
      "The presented approach consists on mainly two tasks. First, a way to translate each data point on the training dataset to an ideal position based on its class assignment. Second, approximate this ideal space transformation as an independet learning task.\n",
      "\n",
      "### Ideal space transformation\n",
      "Let $X$ be a $m \\times n$ matrix with $m$ samples and $n$ attributes, and $Y$ contains $m$ class asignments to datapoins on $X$. $X$ represents $m$ points living on an $n$ dimensional space (we are only concerned on vector spaces over real fields). \n",
      "\n",
      "Let $C_z(A)$ be the centroid of datapoints on $A$ belonging to class $z$. Similarly $C_*(A)$ gives the centroid of all datapoins in $A$.\n",
      "\n",
      "\n",
      "#### Step 1\n",
      "Create a matrix $B$ with each row vector as $C_z(X)$, for each $z$ represented in $Y$\n",
      "\n",
      "#### Step 2\n",
      "Othogonalize matrix $B$ (using Gram-Schmidt or whathever method you want) and normalize. \n",
      "This new orthonormal matrix $B'$ has a unit vector per class linearly independent of other class vectors.\n",
      "\n",
      "#### Step 4\n",
      "Make a new dataset $X'$ with  $x' = x - C_*(X) - C_z(X)*K + B'_z + s$ for each class $z$ present on $Y$\n",
      "\n",
      "\n",
      "Here, $K$ represents a scaling factor based on the spreading of projections of datapoints on a class, over the next class vector.\n",
      "\n",
      "\n",
      "Scaling factor $K$ is dinamicaly set to $K = max(\\frac{ v_i \\cdot b }{\\mid b \\mid})$ for all $v_i$ belonging to previous class, and $b$ is the class vector of actual class. \n",
      "\n",
      "This ensures that no overlapping occurs on any datapoint of one class, over the datapoints on any other class.\n",
      "\n",
      "Now, $X'$ represents an ideal spreading of datapoints.\n",
      "\n",
      "### Approximating ideal space\n",
      "This section is concerned on finding a mapping $f$ such that $f(X) = X'$. Note that $f$ has no information regarding class pertenence.\n",
      "\n",
      "\n",
      "A first approximation to this mapping function $f$ is to assume that $f$ is a linear transformation. On such case, $f$ can be described as a linear operator as $f =  X' \\cdot X^{-1}$.\n",
      "\n",
      "A more complex definition of $f$ can be in terms of an ensamble of regressors, each mapping from all attributes on $X$ to a particular component. That is, if $X \\in R^n$, there are $n$ independent regressors. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Practical examples\n",
      "Lets play with this ClassSparser method "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "_dataset = datasets.load_iris()\n",
      "X = _dataset.data\n",
      "Y = _dataset.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, import `classSparser` class from `classSparser` module. `classSparser` class contains two main methods; `fit()` and `predict()`. On the fit phase, all described method is executed. The ideal space transformation and the labels for each datapoint are returned by `fit()`. This data actually is not needed in practice, but we are going to save these results for visualization purposes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from classSparser import classSparser\n",
      "cS = classSparser(mapperType='Regressor')\n",
      "X2, Y2 = cS.fit(X,Y)\n",
      "X3 = cS.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These previous lines of code are all that is needed in order to perform a transformation of the original space. In this case `X3` contains all datapoints on `X`, after Class Sparser transformation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.manifold import MDS\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "X_a = MDS(n_components=2).fit_transform(X)\n",
      "X_b = MDS(n_components=2).fit_transform(X2)\n",
      "X_c = MDS(n_components=2).fit_transform(X3)\n",
      "\n",
      "\n",
      "fig = plt.figure()\n",
      "a=fig.add_subplot(1,3,1)\n",
      "plt.title(\"original\")\n",
      "plt.scatter(X_a[:, 0], X_a[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "\n",
      "a=fig.add_subplot(1,3,2)\n",
      "plt.title(\"transformed\")\n",
      "plt.scatter(X_b[:, 0], X_b[:, 1], c=Y2, cmap=plt.cm.Paired)\n",
      "\n",
      "a=fig.add_subplot(1,3,3)\n",
      "plt.title(\"approximated\")\n",
      "plt.scatter(X_c[:, 0], X_c[:, 1], c=Y, cmap=plt.cm.Paired)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#References\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}